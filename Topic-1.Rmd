# Topic 1 Exercises


2.4.1\
(1)flexible method better. We have enough training data.\
(2)inflexible method better. Not enough training data. flexible method can overfitting the data.\
(3)flexible method better.Non-linear\
(d)inflexible method better. Flexible method might overfit to the errors.\

2.4.2\
(1) Regression; inference;  n=500 and p=3 \
(2) Classification; prediction;n=20 and p=13 \
(3)Regression; prediction; n=52 and p=3


2.4.3
(1)

![Image for error](IMG_0085.JPG)






(2) Training error declines monotonically as flexibility increases: as we increase parameters to explain the dependent variable, the function fits the training data more closely. \
As flexibility increases, test error declines at first and then starts to increase again: at first larger increasing flexibility leads to more accurate function but then the function would overfitting the data.\
As we use more flexible methods, the variance will increase. Variance refers to the amount by wich our functon would change if we estimated it with another training data set. A very flexible method suggests that the function fit closely with the training data so if we change the training data, the function would change correspondingly, resulting in a big variance. \
Bias will decrease. Bias refers to the error that is introduced by approximating a real-life problem by a much simpler model. There's a trade off between variance and bias. So because variance increases, bias decreases.\
The irreducible error is a constant. Different methods do not change irreducible errors. 



2.6
Parametric methods involve a two-step model based approach: first make ansauumption aboutht efunctional form. Then we use the training data to fit or train the model. Parametric methods reduces the problem of estimating f down to one of estimating a set of parameters. \
Non-parametric methods do not make explicit assumptions about the functional form of f. They seek an estimate of f that gets as close to the data point as possible without being too rough or wiggly. \
Advantages are they can reduce the problem of estimating f to a small number of parameters; do not need a very large number of observations. \
Disadvantages are are they cannot accurately fit a wider range of possible shapes for f. Any parametric aprroach brings with it the possibility that the functional form used to estimate f is very different from the true f, in which case the resulting model will not fit the data well.\


2.7
(a)
1:3
2:2
3:$\sqrt{10}$
4:$\sqrt{5}$
5:$\sqrt{2}$
6:$\sqrt{3}$  \
(b)
Green

(c)
Red
red:$\frac{1}{3}(1+0+1)=\frac{2}{3}$
green:$\frac{1}{3}(0+1+0)=\frac{1}{3}$
It's red.
(d)
Small.
Higher K leads the decision boundary to being linear. Lower K can fit more irregular data like this.

2.8

```{r}
if(!file.exists("~/College.csv")){
    URL <- RCurl::getURL("http://www-bcf.usc.edu/~gareth/ISL/College.csv")
    college <- read.csv(textConnection(URL), header = T)
    rm(URL)
}


```


```{r}
rownames(college)=college[,1]
college=college [,-1]
```


(c)
```{r}
summary(college)
```

```{r}
pairs(college[, 1:10])
```


```{r}
boxplot(college$Outstate ~ college$Private, , main = "Outstate versus Private",  xlab = "Private", ylab = "Outstate")

```

```{r}
Elite=rep("No",nrow(college))
Elite[college$Top10perc >50]="Yes"
Elite=as.factor(Elite)
college=data.frame(college ,Elite)
```


```{r}
E=rep("a",nrow(college))
E[college$Top10perc >50]="Yes"
summary(college$Elite)
boxplot(college$Outstate ~ college$Elite, main = "Outstate versus Elite", xlab = "Elite", ylab = "Outstate")
```
78 ellite colleges.

```{r}
par(mfcol = c(2, 2))
hist(college$Accept, breaks = 8, freq = TRUE, col = "blue", main = "Histogram", xlab = "Accept", ylab = "Value")
hist(college$Accept, breaks = 12, freq = TRUE, col = "green", main = "Histogram", xlab = "Accept", ylab = "Value")
hist(college$Enroll, breaks = 4, freq = TRUE, col = "blue", main = "Histogram", xlab = "Enroll", ylab = "Value")
hist(college$Enroll, breaks = 6, freq = TRUE, col = "green", main = "Histogram", xlab = "Enroll", ylab = "Value")
```

```{r}
pairs(college[, 2:3])
```
Number of applications are positively correlated with number of enrollment.


2.9
```{r}
auto_file_name<-"/home/local/MAC/yli5/math253-assignment/Auto.csv"
auto<-read.csv(auto_file_name)
Auto <- na.omit(auto)
```

(a)
```{r}
str(Auto)
```

horsepower and name are categorical variables and the rest are quantitative. 


```{r}
summary(Auto)
```
mpg:(9,46); Mean   :23.52\
cylinders(3,8); Mean   :5.458\
displacement(68,455); Mean   :193.5\
weight(1613,5140);Mean   :2970 \
acceleration(8,24)； Mean   :15.56\
year (70,82)；Mean   :75.99\
 origin (1,3)； Mean   :1.574  
 
 
```{r}
sd(Auto$mpg)
sd(Auto$cylinders)
sd(Auto$displacement)
sd(Auto$weight)
sd(Auto$acceleration)
sd(Auto$year)
sd(Auto$origin)
```
```{r}
sub<-Auto[-c(10:58),-c(4,9)]
sapply(sub,mean)
sapply(sub,sd)
sapply(sub,range)
```
(e)
```{r}
auto$cylinders <- as.factor(auto$cylinders)
auto$year <- as.factor(auto$year)
auto$origin <- as.factor(auto$origin)
pairs(auto)
```

displacement and weight are linearly correlated.


(f)
cylinder, horsepower and year.